import torch

import os
import numpy as np
from datetime import datetime
import argparse
from utils import _logger, set_requires_grad
from dataloader.dataloader import data_generator
from trainer.trainer import Trainer, model_evaluate
from models.TC import TC
from utils import _calc_metrics, copy_Files
from models.model import base_Model
# Args selections
start_time = datetime.now()


parser = argparse.ArgumentParser()

######################## Model parameters ########################
home_dir = os.getcwd()
parser.add_argument('--experiment_description', default='Exp1', type=str,
                    help='Experiment Description')
parser.add_argument('--run_description', default='run1', type=str,
                    help='Experiment Description')
parser.add_argument('--seed', default=0, type=int,
                    help='seed value')
parser.add_argument('--training_mode', default='supervised', type=str,
                    help='Modes of choice: random_init, supervised, self_supervised, fine_tune, train_linear')
parser.add_argument('--selected_dataset', default='Epilepsy', type=str,
                    help='Dataset of choice: sleepEDF, HAR, Epilepsy, pFD')
parser.add_argument('--logs_save_dir', default='experiments_logs', type=str,
                    help='saving directory')
parser.add_argument('--device', default='cuda', type=str,
                    help='cpu or cuda')
parser.add_argument('--home_path', default=home_dir, type=str,
                    help='Project home directory')
parser.add_argument('--load_path', type=str, default=None, help='Path to the pretrained model checkpoint')
args = parser.parse_args()



device = torch.device(args.device)
experiment_description = args.experiment_description
data_type = args.selected_dataset
method = 'TS-TCC'
training_mode = args.training_mode
run_description = args.run_description

logs_save_dir = args.logs_save_dir
os.makedirs(logs_save_dir, exist_ok=True)


exec(f'from config_files.{data_type}_Configs import Config as Configs')
configs = Configs()

# ##### fix random seeds for reproducibility ########
SEED = args.seed
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.benchmark = False
np.random.seed(SEED)
#####################################################

experiment_log_dir = os.path.join(logs_save_dir, experiment_description, run_description, training_mode + f"_seed_{SEED}")
os.makedirs(experiment_log_dir, exist_ok=True)

# loop through domains
counter = 0
src_counter = 0


# Logging
log_file_name = os.path.join(experiment_log_dir, f"logs_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}.log")
logger = _logger(log_file_name)
logger.debug("=" * 45)
logger.debug(f'Dataset: {data_type}')
logger.debug(f'Method:  {method}')
logger.debug(f'Mode:    {training_mode}')
logger.debug("=" * 45)

# Load datasets
data_path = f"./data/{data_type}"
train_dl, valid_dl, test_dl = data_generator(data_path, configs, training_mode)
logger.debug("Data loaded ...")

# Load Model
model = base_Model(configs).to(device)
temporal_contr_model = TC(configs, device).to(device)

# ================= [ä¿®æ”¹] å¼ºåˆ¶åŠ è½½æƒé‡é€»è¾‘ =================
    # é’ˆå¯¹è¿ç§»å­¦ä¹ ï¼šæ£€æŸ¥å½“å‰å®éªŒç›®å½•ä¸‹æ˜¯å¦æœ‰é¢„å…ˆæ”¾å…¥çš„ ckp_last.pt
chk_path = os.path.join(experiment_log_dir, 'saved_models', 'ckp_last.pt')
    
    # ä»…åœ¨éè‡ªç›‘ç£æ¨¡å¼ä¸‹å°è¯•åŠ è½½ï¼ˆé¿å…é¢„è®­ç»ƒæ—¶è¦†ç›–è‡ªå·±ï¼‰
if training_mode != "self_supervised":
        if os.path.exists(chk_path):
            logger.debug(f"ğŸ”¥ [Transfer Learning] Found checkpoint: {chk_path}")
            logger.debug("Loading pre-trained weights...")
            
            checkpoint = torch.load(chk_path)
            
            # å…¼å®¹å¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ 'model_state_dict' é”®
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint.keys():
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
                
            # åŠ è½½å‚æ•° (strict=False å…è®¸åˆ†ç±»å¤´ä¸åŒ¹é…ï¼Œè¿™æ˜¯è¿ç§»å­¦ä¹ çš„å…³é”®)
            try:
                model.load_state_dict(state_dict, strict=False)
                logger.debug("âœ… Pre-trained weights loaded successfully!")
            except Exception as e:
                logger.debug(f"âš ï¸ Warning during loading weights: {e}")
        else:
            logger.debug(f"No checkpoint found at {chk_path}. Training from scratch (Random Init).")
    # ================= [ä¿®æ”¹ç»“æŸ] =================


if training_mode == "fine_tune":
    # load saved model of this experiment
    load_from = os.path.join(os.path.join(logs_save_dir, experiment_description, run_description, f"self_supervised_seed_{SEED}", "saved_models"))
    chkpoint = torch.load(os.path.join(load_from, "ckp_last.pt"), map_location=device)
    pretrained_dict = chkpoint["model_state_dict"]
    model_dict = model.state_dict()
    del_list = ['logits']
    pretrained_dict_copy = pretrained_dict.copy()
    for i in pretrained_dict_copy.keys():
        for j in del_list:
            if j in i:
                del pretrained_dict[i]
    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)

if training_mode == "train_linear" or "tl" in training_mode:
    load_from = os.path.join(os.path.join(logs_save_dir, experiment_description, run_description, f"self_supervised_seed_{SEED}", "saved_models"))
    chkpoint = torch.load(os.path.join(load_from, "ckp_last.pt"), map_location=device)
    pretrained_dict = chkpoint["model_state_dict"]
    model_dict = model.state_dict()

    # 1. filter out unnecessary keys
    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}

    # delete these parameters (Ex: the linear layer at the end)
    del_list = ['logits']
    pretrained_dict_copy = pretrained_dict.copy()
    for i in pretrained_dict_copy.keys():
        for j in del_list:
            if j in i:
                del pretrained_dict[i]

    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)
    set_requires_grad(model, pretrained_dict, requires_grad=False)  # Freeze everything except last layer.

if training_mode == "random_init":
    model_dict = model.state_dict()

    # delete all the parameters except for logits
    del_list = ['logits']
    pretrained_dict_copy = model_dict.copy()
    for i in pretrained_dict_copy.keys():
        for j in del_list:
            if j in i:
                del model_dict[i]
    set_requires_grad(model, model_dict, requires_grad=False)  # Freeze everything except last layer.


# ======================================================
# ğŸ”§ [ç§‘ç ”ç¼–ç¨‹] ä¼˜åŒ–å™¨ç­–ç•¥ä¿®æ”¹ï¼šå¾®è°ƒæ¨¡å¼é™æƒä¿æŠ¤
#    (å°†è¿™æ®µä»£ç æ›¿æ¢æ‰åŸæ¥çš„ model_optimizer = ... )
# ======================================================

if training_mode == "fine_tune":
    # ç­–ç•¥ï¼šå¾®è°ƒæ—¶ï¼Œä¸»å¹²ç½‘ç»œéå¸¸è„†å¼±ï¼Œå¿…é¡»ç”¨æå°çš„å­¦ä¹ ç‡å‘µæŠ¤
    # é€šå¸¸å»ºè®®ä¸ºåŸå­¦ä¹ ç‡çš„ 1/10 ç”šè‡³ 1/100
    ft_lr = configs.lr * 1  # å»ºè®®å…ˆè¯• 0.01 (é™ä½100å€) æˆ–è€… 0.1 (é™ä½10å€)
    configs.num_epoch = 100
    
    print(f"\nğŸ›¡ï¸ [Strategy] æ¿€æ´»å¾®è°ƒä¿æŠ¤æœºåˆ¶ï¼š")
    print(f"   - åŸå§‹å­¦ä¹ ç‡: {configs.lr}")
    print(f"   - å¾®è°ƒå­¦ä¹ ç‡: {ft_lr}")
    print(f"   - ç›®çš„: é˜²æ­¢éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ç ´åé¢„è®­ç»ƒç‰¹å¾\n")
    
    model_optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=ft_lr, 
        betas=(configs.beta1, configs.beta2), 
        weight_decay=3e-4
    )
else:
    # ç›‘ç£å­¦ä¹ (è“çº¿) æˆ– è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä¿æŒç«åŠ›å…¨å¼€
    model_optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=configs.lr, 
        betas=(configs.beta1, configs.beta2), 
        weight_decay=3e-4
    )
# ======================================================


temporal_contr_optimizer = torch.optim.Adam(temporal_contr_model.parameters(), lr=configs.lr, betas=(configs.beta1, configs.beta2), weight_decay=3e-4)

if training_mode == "self_supervised":  # to do it only once
    copy_Files(os.path.join(logs_save_dir, experiment_description, run_description), data_type)

# =========================================================
# ğŸ“¢ [æ–°ç‰ˆ] å¼ºè¡ŒåŠ è½½æŒ‡å®šè·¯å¾„çš„æ¨¡å‹
# =========================================================
if args.load_path:
    load_path = args.load_path
    print(f"\nğŸ” [æŒ‡ä»¤] æ­£åœ¨åŠ è½½æŒ‡å®šæ¨¡å‹: {load_path}")

    if os.path.exists(load_path):
        checkpoint = torch.load(load_path)
        state_dict = checkpoint.get('model_state_dict', checkpoint)

        # è¿‡æ»¤æ‰åˆ†ç±»å¤´ (logits)ï¼ŒåªåŠ è½½ç‰¹å¾æå–å™¨
        state_dict = {k: v for k, v in state_dict.items() if 'logits' not in k}

        # åŠ è½½
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        print(f"âœ… [æˆåŠŸ] é¢„è®­ç»ƒæƒé‡å·²åŠ è½½ï¼(Missing keys: {len(missing)})")
        logger.debug(f"Loaded weights from {load_path}")
    else:
        print(f"âŒ [é”™è¯¯] æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {load_path}")
        # è¿™é‡Œå¦‚æœä¸é€€å‡ºï¼Œå°±ä¼šå˜æˆéšæœºåˆå§‹åŒ–ï¼Œä¸€å®šè¦æ³¨æ„
# =========================================================

# Trainer(...) # åŸæœ‰ä»£ç 
    
# Trainer
Trainer(model, temporal_contr_model, model_optimizer, temporal_contr_optimizer, train_dl, valid_dl, test_dl, device, logger, configs, experiment_log_dir, training_mode)

if training_mode != "self_supervised":
    # Testing
    outs = model_evaluate(model, temporal_contr_model, test_dl, device, training_mode)
    total_loss, total_acc, pred_labels, true_labels = outs
    _calc_metrics(pred_labels, true_labels, experiment_log_dir, args.home_path)

logger.debug(f"Training time is : {datetime.now()-start_time}")
