import os
import sys

sys.path.append("..")
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from models.loss import NTXentLoss



def Trainer(model, temporal_contr_model, model_optimizer, temp_cont_optimizer, train_dl, valid_dl, test_dl, device, logger, config, experiment_log_dir, training_mode):
    # Start training
    logger.debug("Training started ....")

# =======================================================
    # ã€åœ¨è¿™é‡Œæ’å…¥ã€‘ æƒé‡åˆ¤æ–­é€»è¾‘
    # =======================================================
    # 1. åˆå§‹åŒ–é»˜è®¤æƒé‡ (å¯¹åº”åŸå§‹è®ºæ–‡/è·¨è§†å›¾)
    config.lambda1 = 1.0       # Cross-View æƒé‡
    config.lambda2 = 0.7     # Context æƒé‡
    config.lambda_self = 0  # Self-View æƒé‡ (é»˜è®¤å…³é—­)

   # è·å–æ–‡ä»¶å¤¹åç§°çš„å°å†™å½¢å¼ï¼Œæ–¹ä¾¿åˆ¤æ–­
    run_name = experiment_log_dir.lower()

    if "mixed" in run_name:
        # Mixed æ¨¡å¼
        config.lambda_self = 0.7
        print(f"ğŸ‘‰ æ£€æµ‹åˆ° Mixed æ¨¡å¼ (lambda1={config.lambda1}, lambda_self={config.lambda_self})")
        
    elif "cross" in run_name:
        # Cross æ¨¡å¼ (ä¿æŒé»˜è®¤)
        config.lambda1 = 1.0
        config.lambda_self = 0
        print(f"ğŸ‘‰ æ£€æµ‹åˆ° Cross æ¨¡å¼ (lambda1={config.lambda1}, lambda_self={config.lambda_self})")
        
    elif "self" in run_name:
        # Self æ¨¡å¼ (åªæœ‰å½“æ—¢ä¸æ˜¯mixedä¹Ÿä¸æ˜¯crossï¼Œä¸”åŒ…å«selfæ—¶ï¼Œæ‰è¿›è¿™é‡Œ)
        config.lambda1 = 0 
        config.lambda_self = 1 
        print(f"ğŸ‘‰ æ£€æµ‹åˆ° Self æ¨¡å¼ (lambda1={config.lambda1}, lambda_self={config.lambda_self})")
    
    criterion = nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, 'min')

    for epoch in range(1, config.num_epoch + 1):
        # Train and validate
        train_loss, train_acc = model_train(model, temporal_contr_model, model_optimizer, temp_cont_optimizer, criterion, train_dl, config, device, training_mode)
        valid_loss, valid_acc, _, _ = model_evaluate(model, temporal_contr_model, valid_dl, device, training_mode)
        if training_mode != 'self_supervised':  # use scheduler in all other modes.
            scheduler.step(valid_loss)

        logger.debug(f'\nEpoch : {epoch}\n'
                     f'Train Loss     : {train_loss:.4f}\t | \tTrain Accuracy     : {train_acc:2.4f}\n'
                     f'Valid Loss     : {valid_loss:.4f}\t | \tValid Accuracy     : {valid_acc:2.4f}')

    os.makedirs(os.path.join(experiment_log_dir, "saved_models"), exist_ok=True)
    chkpoint = {'model_state_dict': model.state_dict(), 'temporal_contr_model_state_dict': temporal_contr_model.state_dict()}
    torch.save(chkpoint, os.path.join(experiment_log_dir, "saved_models", f'ckp_last.pt'))

    if training_mode != "self_supervised":  # no need to run the evaluation for self-supervised mode.
        # evaluate on the test set
        logger.debug('\nEvaluate on the Test set:')
        test_loss, test_acc, _, _ = model_evaluate(model, temporal_contr_model, test_dl, device, training_mode)
        logger.debug(f'Test loss      :{test_loss:0.4f}\t | Test Accuracy      : {test_acc:0.4f}')

    logger.debug("\n################## Training is Done! #########################")


def model_train(model, temporal_contr_model, model_optimizer, temp_cont_optimizer, criterion, train_loader, config, device, training_mode):
    total_loss = []
    total_acc = []
    model.train()
    temporal_contr_model.train()

    for batch_idx, (data, labels, aug1, aug2) in enumerate(train_loader):
        # 1. æ•°æ®æ¬è¿
        data, labels = data.float().to(device), labels.long().to(device)
        aug1, aug2 = aug1.float().to(device), aug2.float().to(device)

        # 2. æ¢¯åº¦æ¸…é›¶
        model_optimizer.zero_grad()
        temp_cont_optimizer.zero_grad()

        # =========================================================
        # åˆ†æ”¯ A: è‡ªç›‘ç£é¢„è®­ç»ƒ (Self-Supervised) â€”â€” ã€ä¿®æ”¹æ ¸å¿ƒã€‘
        # =========================================================
        if training_mode == "self_supervised":
            # è·å–ç‰¹å¾
            predictions1, features1 = model(aug1)
            predictions2, features2 = model(aug2)

            # å½’ä¸€åŒ–
            features1 = F.normalize(features1, dim=1)
            features2 = F.normalize(features2, dim=1)

            # -------------------------------------------------------
            # 1. è®¡ç®—ã€è·¨è§†å›¾ã€‘TC æŸå¤± (Original / Cross-View)
            # -------------------------------------------------------
            # é€»è¾‘ï¼šç”¨ View1 çš„ä¸Šä¸‹æ–‡é¢„æµ‹ View2 çš„æœªæ¥ï¼ˆåŠå…¶åå‘ï¼‰
            # ä½œç”¨ï¼šå­¦ä¹ å¯¹å™ªå£°å’Œå¢å¼ºçš„ä¸å˜æ€§ (Invariance)
            tc_loss_cross1, context1 = temporal_contr_model(features1, features2)
            tc_loss_cross2, context2 = temporal_contr_model(features2, features1)
            
            # -------------------------------------------------------
            # 2. è®¡ç®—ã€åŒè§†å›¾ã€‘TC æŸå¤± (New / Same-View)
            # -------------------------------------------------------
            # é€»è¾‘ï¼šç”¨ View1 çš„ä¸Šä¸‹æ–‡é¢„æµ‹ View1 è‡ªå·±çš„æœªæ¥ï¼ˆåŠå…¶åå‘ï¼‰
            # ä½œç”¨ï¼šåŠ å¼ºå¯¹å•ä¸€æ ·æœ¬å†…éƒ¨æ—¶åºä¾èµ–çš„å­¦ä¹  (Temporal Dependency)
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦è¿”å›çš„ä¸Šä¸‹æ–‡ contextï¼Œç”¨ _ å¿½ç•¥
            tc_loss_self1, _ = temporal_contr_model(features1, features1)
            tc_loss_self2, _ = temporal_contr_model(features2, features2)

            # -------------------------------------------------------
            # 3. è®¡ç®—ã€ä¸Šä¸‹æ–‡ã€‘CC æŸå¤± (Contextual Contrasting)
            # -------------------------------------------------------
            # ä½¿ç”¨è·¨è§†å›¾äº§ç”Ÿçš„ä¸Šä¸‹æ–‡å‘é‡è®¡ç®—ä¸€è‡´æ€§
            zis = context1 
            zjs = context2 
            
            nt_xent_criterion = NTXentLoss(device, config.batch_size, config.Context_Cont.temperature,
                                           config.Context_Cont.use_cosine_similarity)
            loss_cc = nt_xent_criterion(zis, zjs)

            # -------------------------------------------------------
            # 4. ç»„åˆæ€»æŸå¤± (Joint Loss)
            # -------------------------------------------------------
            lambda1 = config.lambda1       # è·¨è§†å›¾æƒé‡ (å»ºè®®ä¿æŒä¸»å¯¼)
            lambda2 = config.lambda2    # ä¸Šä¸‹æ–‡æƒé‡
            lambda_self = config.lambda_self # ã€æ–°æƒé‡ã€‘åŒè§†å›¾æƒé‡ (å»ºè®®è®¾å°ä¸€ç‚¹ï¼Œé¿å…æ¨¡å‹å·æ‡’)

            # æ€»æŸå¤± = (è·¨è§†å›¾TC) + (åŒè§†å›¾TC) + (ä¸Šä¸‹æ–‡CC)
            loss = (tc_loss_cross1 + tc_loss_cross2) * lambda1 + \
                   (tc_loss_self1 + tc_loss_self2) * lambda_self + \
                   loss_cc * lambda2

        # =========================================================
        # åˆ†æ”¯ B: ç›‘ç£/å¾®è°ƒ (Supervised) â€”â€” ã€ä¿æŒåŸæ ·ã€‘
        # =========================================================
        else: 
            output = model(data)
            predictions, features = output
            loss = criterion(predictions, labels) # ä»…è®¡ç®—åˆ†ç±»æŸå¤±
            total_acc.append(labels.eq(predictions.detach().argmax(dim=1)).float().mean())

        # åå‘ä¼ æ’­
        total_loss.append(loss.item())
        loss.backward()
        model_optimizer.step()
        temp_cont_optimizer.step()

    total_loss = torch.tensor(total_loss).mean()

    if training_mode == "self_supervised":
        total_acc = 0
    else:
        total_acc = torch.tensor(total_acc).mean()
        
    return total_loss, total_acc


def model_evaluate(model, temporal_contr_model, test_dl, device, training_mode):
    model.eval()
    temporal_contr_model.eval()

    total_loss = []
    total_acc = []

    criterion = nn.CrossEntropyLoss()
    outs = np.array([])
    trgs = np.array([])

    with torch.no_grad():
        for data, labels, _, _ in test_dl:
            data, labels = data.float().to(device), labels.long().to(device)

            if training_mode == "self_supervised":
                pass
            else:
                output = model(data)

            # compute loss
            if training_mode != "self_supervised":
                predictions, features = output
                loss = criterion(predictions, labels)
                total_acc.append(labels.eq(predictions.detach().argmax(dim=1)).float().mean())
                total_loss.append(loss.item())

            if training_mode != "self_supervised":
                pred = predictions.max(1, keepdim=True)[1]  # get the index of the max log-probability
                outs = np.append(outs, pred.cpu().numpy())
                trgs = np.append(trgs, labels.data.cpu().numpy())

    if training_mode != "self_supervised":
        total_loss = torch.tensor(total_loss).mean()  # average loss
    else:
        total_loss = 0
    if training_mode == "self_supervised":
        total_acc = 0
        return total_loss, total_acc, [], []
    else:
        total_acc = torch.tensor(total_acc).mean()  # average acc
    return total_loss, total_acc, outs, trgs
